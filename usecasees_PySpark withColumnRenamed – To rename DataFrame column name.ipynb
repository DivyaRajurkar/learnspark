{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9141a43a-6cff-4819-88b9-dfb89fa4b52c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing library \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9453aaa-b0db-4560-9482-0300dc5de7d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: '\\ndf = spark.createDataFrame(data = dataDF, schema = schema)\\ndf.printSchema()\\n'"
     ]
    }
   ],
   "source": [
    "# Create a Spark session and DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples').getOrCreate()\n",
    "'''\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6b7920-7ed4-4122-8147-947c6aa2f752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema with nested structure\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('dob', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6fcc1f-a94e-4d55-b2ca-06a16a7e1a49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example Data\n",
    "dataDF = [\n",
    "    (('James', '', 'Smith'), '1991-04-01', 'M', 3000),\n",
    "    (('Michael', 'Rose', ''), '2000-05-19', 'M', 4000),\n",
    "    (('Robert', '', 'Williams'), '1978-09-05', 'M', 4000),\n",
    "    (('Maria', 'Anne', 'Jones'), '1967-12-01', 'F', 4000),\n",
    "    (('Jen', 'Mary', 'Brown'), '1980-02-17', 'F', -1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ced7003a-90d0-4951-bdab-b7541a912744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    " #DataFrame\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c4a05e2b-b988-4ecd-adaf-7174815468eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+------+\n|                name|       dob|gender|salary|\n+--------------------+----------+------+------+\n|    {James, , Smith}|1991-04-01|     M|  3000|\n|   {Michael, Rose, }|2000-05-19|     M|  4000|\n|{Robert, , Williams}|1978-09-05|     M|  4000|\n|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n+--------------------+----------+------+------+\n\nOut[28]: '\\nThe df.show() method in PySpark displays the content of a DataFrame in a tabular format. By default, it shows up to 20 rows of the DataFrame and truncates strings longer than 20 characters\\n'"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "'''\n",
    "The df.show() method in PySpark displays the content of a DataFrame in a tabular format. By default, it shows up to 20 rows of the DataFrame and truncates strings longer than 20 characters\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b5dc2a89-2a84-4fea-bc0c-597f155e3802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+----------+------+------+\n|                name|       dob|gender|salary|\n+--------------------+----------+------+------+\n|    {James, , Smith}|1991-04-01|     M|  3000|\n|   {Michael, Rose, }|2000-05-19|     M|  4000|\n|{Robert, , Williams}|1978-09-05|     M|  4000|\n|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n+--------------------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#usecase1. PySpark withColumnRenamed – To rename a DataFrame column name\n",
    "df.withColumnRenamed(\"dob\", \"DateOfBirth\").printSchema()\n",
    "df.show(truncate=True)#byfalut show 20 charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "21cb9317-3765-4c8b-bb0b-c78736e2015f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# UseCase2. PySpark withColumnRenamed – To rename multiple columns\n",
    "df2 = df.withColumnRenamed(\"dob\", \"DateOfBirth\") \\\n",
    "    .withColumnRenamed(\"salary\", \"salary_amount\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6b233782-9960-4252-820a-aa3c8020db5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UseCase3 Using PySpark StructType – To rename a nested column in Dataframe\n",
    "Changing a column name on nested data is not straight forward and we can do this by creating a new schema with new DataFrame columns using StructType and use it using cast function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f823b89-5228-479c-bea1-19e8ec4784c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nOut[32]: ' df.select(col(\"name\").cast(schema2),      col(\"dob\"), col(\"gender\"),col(\"salary\"))    .printSchema()  '"
     ]
    }
   ],
   "source": [
    "# UseCase3. Using PySpark StructType – To rename a nested column in DataFrame\n",
    "#from pyspark.sql.functions import col\n",
    "\n",
    "# Define the new schema for the nested structure\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Select columns, casting the nested 'name' field to the new schema\n",
    "df.select(col(\"name\").cast(schema2), col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()\n",
    "\n",
    "''' df.select(col(\"name\").cast(schema2), \\\n",
    "     col(\"dob\"), col(\"gender\"),col(\"salary\")) \\\n",
    "   .printSchema()  '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a663a65-94de-4985-8cb5-181dea6f5252",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# UseCase4. Using Select – To rename nested elements\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"), \n",
    "          col(\"name.middlename\").alias(\"mname\"), \n",
    "          col(\"name.lastname\").alias(\"lname\"), \n",
    "          col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "70000f38-8aef-483d-965b-649cb7cfa365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UseCase5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "When you have nested columns on PySpark DatFrame and if you want to rename it, use withColumn on a data frame object to create a new column from an existing and we will need to drop the existing column. Below example creates a “fname” column from “name.firstname” and drops the “name” column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bc9589-3661-4829-a56c-ec4a9a3a13cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n\n+----------+------+------+-------+-----+--------+\n|       dob|gender|salary|  fname|mname|   lname|\n+----------+------+------+-------+-----+--------+\n|1991-04-01|     M|  3000|  James|     |   Smith|\n|2000-05-19|     M|  4000|Michael| Rose|        |\n|1978-09-05|     M|  4000| Robert|     |Williams|\n|1967-12-01|     F|  4000|  Maria| Anne|   Jones|\n|1980-02-17|     F|    -1|    Jen| Mary|   Brown|\n+----------+------+------+-------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "df4 = df.withColumn(\"fname\", col(\"name.firstname\")) \\\n",
    "        .withColumn(\"mname\", col(\"name.middlename\")) \\\n",
    "        .withColumn(\"lname\", col(\"name.lastname\")) \\\n",
    "        .drop(\"name\")\n",
    "df4.printSchema()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68b3e9b-ba3d-421d-9b4d-4699bb742e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- income: integer (nullable = true)\n\n+----------+------+------+-------+-----+--------+\n|       dob|gender|salary|  fname|mname|   lname|\n+----------+------+------+-------+-----+--------+\n|1991-04-01|     M|  3000|  James|     |   Smith|\n|2000-05-19|     M|  4000|Michael| Rose|        |\n|1978-09-05|     M|  4000| Robert|     |Williams|\n|1967-12-01|     F|  4000|  Maria| Anne|   Jones|\n|1980-02-17|     F|    -1|    Jen| Mary|   Brown|\n+----------+------+------+-------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Using col() function – To Dynamically rename all or multiple columns\n",
    "newColumns = [col(\"name.firstname\").alias(\"fname\"),\n",
    "              col(\"name.middlename\").alias(\"mname\"),\n",
    "              col(\"name.lastname\").alias(\"lname\"),\n",
    "              col(\"dob\").alias(\"DateOfBirth\"),\n",
    "              col(\"gender\").alias(\"sex\"),\n",
    "              col(\"salary\").alias(\"income\")]\n",
    "df6 = df.select(*newColumns)\n",
    "df6.printSchema()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b468f009-003e-46cd-a92b-3740b708fd35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note\n",
    "# in usecase5To rename nested columns \n",
    "\n",
    "we used two method as we see in usecasecase5 reate new df then yse df.withColumn(\"new_column\", col_expression) then drop existing column.\n",
    "\n",
    "# in usecase 6. Using col() function – To Dynamically rename all or multiple columns\n",
    "\n",
    "Each element of the newColumns list is a column expression created using col() function to reference specific columns from the DataFrame, with an alias assigned using the alias() method. This allows you to rename the columns while selecting them or creating new ones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36594ae3-ef34-4d71-8622-bfb25096fcff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n+--------------------+----------+------+------+\n|                name|       dob|gender|salary|\n+--------------------+----------+------+------+\n|    {James, , Smith}|1991-04-01|     M|  3000|\n|   {Michael, Rose, }|2000-05-19|     M|  4000|\n|{Robert, , Williams}|1978-09-05|     M|  4000|\n|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n+--------------------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#usecase7. Using toDF() – To change all columns in a PySpark DataFrame\n",
    "'''\n",
    "When we have data in a flat structure (without nested) , use toDF() with a new schema to change all column names.\n",
    "'''\n",
    "#code\n",
    "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*newColumns).printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f32566f-e924-4e39-b12d-3067b8fcd432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- income: integer (nullable = true)\n\nroot\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n+--------------------+----------+------+------+\n|                name|       dob|gender|salary|\n+--------------------+----------+------+------+\n|    {James, , Smith}|1991-04-01|     M|  3000|\n|   {Michael, Rose, }|2000-05-19|     M|  4000|\n|{Robert, , Williams}|1978-09-05|     M|  4000|\n|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n+--------------------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Example Data\n",
    "dataDF = [\n",
    "    (('James', '', 'Smith'), '1991-04-01', 'M', 3000),\n",
    "    (('Michael', 'Rose', ''), '2000-05-19', 'M', 4000),\n",
    "    (('Robert', '', 'Williams'), '1978-09-05', 'M', 4000),\n",
    "    (('Maria', 'Anne', 'Jones'), '1967-12-01', 'F', 4000),\n",
    "    (('Jen', 'Mary', 'Brown'), '1980-02-17', 'F', -1)\n",
    "]\n",
    "\n",
    "# Define the schema with nested structure\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('dob', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a Spark session and DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()\n",
    "\n",
    "# 1. PySpark withColumnRenamed – To rename a DataFrame column name\n",
    "df.withColumnRenamed(\"dob\", \"DateOfBirth\").printSchema()\n",
    "\n",
    "# 2. PySpark withColumnRenamed – To rename multiple columns\n",
    "df2 = df.withColumnRenamed(\"dob\", \"DateOfBirth\") \\\n",
    "    .withColumnRenamed(\"salary\", \"salary_amount\")\n",
    "df2.printSchema()\n",
    "\n",
    "# 3. Using PySpark StructType – To rename a nested column in DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.select(col(\"name\").cast(schema2), col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()\n",
    "\n",
    "# 4. Using Select – To rename nested elements\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"), \n",
    "          col(\"name.middlename\").alias(\"mname\"), \n",
    "          col(\"name.lastname\").alias(\"lname\"), \n",
    "          col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()\n",
    "\n",
    "# 5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "df4 = df.withColumn(\"fname\", col(\"name.firstname\")) \\\n",
    "        .withColumn(\"mname\", col(\"name.middlename\")) \\\n",
    "        .withColumn(\"lname\", col(\"name.lastname\")) \\\n",
    "        .drop(\"name\")\n",
    "df4.printSchema()\n",
    "\n",
    "# 6. Using col() function – To Dynamically rename all or multiple columns\n",
    "newColumns = [col(\"name.firstname\").alias(\"fname\"),\n",
    "              col(\"name.middlename\").alias(\"mname\"),\n",
    "              col(\"name.lastname\").alias(\"lname\"),\n",
    "              col(\"dob\").alias(\"DateOfBirth\"),\n",
    "              col(\"gender\").alias(\"sex\"),\n",
    "              col(\"salary\").alias(\"income\")]\n",
    "df6 = df.select(*newColumns)\n",
    "df6.printSchema()\n",
    "\n",
    "# 7. Using toDF() – To change all columns in a PySpark DataFrame\n",
    "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*newColumns).printSchema() #(*newColumns) error withoutbracket\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "usecasees_PySpark withColumnRenamed – To rename DataFrame column name",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
